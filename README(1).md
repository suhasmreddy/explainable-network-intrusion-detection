# ğŸ›¡ï¸ Explainable AI for Network Intrusion Detection

An end-to-end machine learning pipeline for classifying network traffic into **5 attack categories** (Normal, DoS, Probe, R2L, U2R) using the NSL-KDD benchmark dataset, with full model explainability via SHAP.

---

## ğŸ“‹ Project Overview

Network Intrusion Detection Systems (NIDS) are critical infrastructure in cybersecurity â€” but most ML-based detectors operate as black boxes. Security analysts can't act on alerts they don't understand.

This project builds a **transparent, explainable** intrusion detection system that not only classifies network connections with high accuracy but also provides **per-prediction explanations** of *why* a connection was flagged, using SHAP (SHapley Additive exPlanations).

### Key Results

| Metric | Random Forest (Baseline) | XGBoost (Optimized) | Improvement |
|--------|--------------------------|---------------------|-------------|
| **Accuracy** | 0.7403 | 0.7893 | +4.9% |
| **Macro F1** | 0.5234 | 0.6156 | +17.6% relative |
| **Weighted F1** | 0.6930 | 0.7549 | +6.2% |

### Per-Class Recall Improvement

| Attack Class | Baseline | XGBoost | Change |
|-------------|----------|---------|--------|
| DoS | 77.1% | 83.8% | â†‘ +6.7% |
| Normal | 97.1% | 97.2% | â†‘ +0.1% |
| Probe | 61.3% | 73.9% | â†‘ +12.6% |
| R2L | 0.5% | 10.3% | â†‘ +9.8% |
| U2R | 17.9% | 29.9% | â†‘ +11.9% |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA INGESTION                            â”‚
â”‚  NSL-KDD Dataset (125,973 train / 22,544 test)              â”‚
â”‚  41 features Â· 23 attack types Â· 5 categories               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                EXPLORATORY DATA ANALYSIS                     â”‚
â”‚  Class distribution Â· Feature correlations Â· Statistical     â”‚
â”‚  profiling Â· Normal vs Attack distribution analysis          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FEATURE ENGINEERING                           â”‚
â”‚  16 new features: traffic ratios, error composites,          â”‚
â”‚  log transforms, behavioral flags, interaction terms         â”‚
â”‚  Final: 57 features                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PREPROCESSING PIPELINE                          â”‚
â”‚  Label encoding (categoricals) Â· StandardScaler (fitted on   â”‚
â”‚  train only) Â· SMOTE oversampling (R2L: 995â†’8K, U2R: 52â†’4K) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODELING                                   â”‚
â”‚  Baseline: Random Forest (200 trees, balanced weights)       â”‚
â”‚  Final: XGBoost + Optuna (40-trial Bayesian HPO)             â”‚
â”‚  Optimized for Macro F1 via 3-fold stratified CV             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 EXPLAINABILITY (SHAP)                         â”‚
â”‚  Global: Feature importance bar + beeswarm plots (all 5      â”‚
â”‚  classes) Â· Local: Waterfall plots for individual predictions â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Technical Details

### Dataset: NSL-KDD
The NSL-KDD dataset is the de facto benchmark for evaluating network intrusion detection systems. It is a cleaned and improved version of the original KDD Cup '99 dataset, addressing issues of redundant records and class imbalance.

**5 Attack Categories:**
- **Normal** â€” Legitimate network traffic
- **DoS** (Denial of Service) â€” Flood attacks that overwhelm a target (e.g., neptune, smurf)
- **Probe** â€” Surveillance and port scanning (e.g., satan, ipsweep, nmap)
- **R2L** (Remote-to-Local) â€” Unauthorized remote access (e.g., guess_passwd, ftp_write)
- **U2R** (User-to-Root) â€” Privilege escalation (e.g., buffer_overflow, rootkit)

### Feature Engineering (16 new features)
| Category | Features | Rationale |
|----------|----------|-----------|
| Traffic Volume | `total_bytes`, `src_dst_byte_ratio`, `log_src_bytes`, `log_dst_bytes`, `log_total_bytes` | DoS floods show extreme send/receive imbalance |
| Error Composites | `serror_composite`, `rerror_composite` | Consolidates 4 highly correlated (r>0.95) error rate features into single scores |
| Behavioral Flags | `has_serror`, `has_rerror`, `zero_src_bytes`, `zero_dst_bytes`, `has_compromise_indicators` | Binary indicators of suspicious patterns |
| Interaction Terms | `same_srv_rate_x_count`, `dst_host_srv_diversity`, `dst_host_concentration`, `srv_count_ratio` | Captures non-linear feature relationships |

### Class Imbalance Handling
SMOTE (Synthetic Minority Oversampling Technique) was applied to training data only:
- R2L: 995 â†’ 8,000 samples (+7,005 synthetic)
- U2R: 52 â†’ 4,000 samples (+3,948 synthetic)

### Hyperparameter Optimization
Optuna Bayesian optimization (40 trials) over 9 hyperparameters, optimizing Macro F1 via 3-fold stratified cross-validation. Best CV score: **0.9986**.

### SHAP Explainability Insights
- **DoS detection** driven by: `dst_host_srv_count`, `count`, connection error rates
- **Probe detection** driven by: `total_bytes`, `dst_host_diff_srv_rate`, service diversity
- **R2L detection** driven by: `count`, `service`, `logged_in`, `dst_host_same_src_port_rate`
- **U2R detection** driven by: `dst_host_srv_count`, `duration`, `logged_in`, `num_file_creations`

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Language | Python 3.10+ |
| Data Manipulation | Pandas, NumPy |
| Visualization | Matplotlib, Seaborn |
| ML Framework | Scikit-Learn, XGBoost |
| Imbalanced Learning | imbalanced-learn (SMOTE) |
| Hyperparameter Tuning | Optuna (Bayesian TPE) |
| Explainability | SHAP (TreeExplainer) |
| Environment | Google Colab / Jupyter Notebook |

---

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/explainable-ids.git
cd explainable-ids

# Install dependencies
pip install pandas numpy matplotlib seaborn scikit-learn xgboost shap optuna imbalanced-learn

# Run the notebook
jupyter notebook explainable_ids.ipynb
```

---

## ğŸ“ Project Structure

```
explainable-ids/
â”œâ”€â”€ explainable_ids.ipynb    # Full end-to-end notebook
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ figures/                 # Saved visualizations
    â”œâ”€â”€ correlation_heatmap.png
    â”œâ”€â”€ class_distribution.png
    â”œâ”€â”€ confusion_matrix_comparison.png
    â”œâ”€â”€ shap_global_importance.png
    â”œâ”€â”€ shap_beeswarm_dos.png
    â”œâ”€â”€ shap_beeswarm_probe.png
    â”œâ”€â”€ shap_beeswarm_r2l.png
    â”œâ”€â”€ shap_beeswarm_u2r.png
    â””â”€â”€ project_summary_dashboard.png
```

---

## ğŸ“ Limitations & Future Work

- **R2L and U2R remain challenging** â€” These attack types intentionally mimic normal traffic patterns, making them inherently difficult to detect. This is a well-documented challenge in the NSL-KDD literature.
- **SMOTE limitations** â€” Synthetic oversampling for extremely small classes (U2R: 52 samples) may not capture the full diversity of real attack variants.
- **Future improvements:** Deep learning approaches (LSTM/Transformer for sequential connection patterns), ensemble stacking, and evaluation on the CICIDS2017 dataset for modern attack coverage.

---

## ğŸ“œ License

This project is open-source under the MIT License.

---

## ğŸ™ Acknowledgments

- [NSL-KDD Dataset](https://www.unb.ca/cic/datasets/nsl.html) â€” Canadian Institute for Cybersecurity
- [SHAP Library](https://github.com/shap/shap) â€” Lundberg & Lee, 2017
- [XGBoost](https://xgboost.readthedocs.io/) â€” Chen & Guestrin, 2016
- [Optuna](https://optuna.org/) â€” Akiba et al., 2019
